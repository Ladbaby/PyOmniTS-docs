---
sidebar_position: 1
---

# ğŸš€ Get Started

This tutorial guides you running experiments.

## 1. â¬ Clone the Repository

```shell
cd /path/to/your/project
git clone https://github.com/Ladbaby/PyOmniTS.git
```

## 2. ğŸ’¿ Prepare the Environment

1. Create a new Python virtual environment via the tool of your choice, and activate it. For example, using [Miniconda](https://docs.conda.io/en/latest/miniconda.html)/[Anaconda](https://www.anaconda.com/):

    ```
    conda create -n pyomnits python=3.12
    conda activate pyomnits
    ```
    Python 3.10~3.12 have been tested.

2. Install dependencies.

    Choose one of the options:

    - Option 1: Fuzzy package versions, the legacy way.

        ```shell
        pip install -r requirements.txt
        ```

        > ğŸ’¡ For faster installation speed, consider installing [uv](https://github.com/astral-sh/uv) and running `uv pip install -r requirements.txt` instead.
    - Option 2: Exact package versions, the aggressive way.

        > âš ï¸ It assumes your Linux server to have cuda version 12, which can be less flexible than option 1.

        Install [uv](https://github.com/astral-sh/uv), then:

        ```shell
        uv pip sync requirements.lock
        ```

    > ğŸ”¥Note: some packages are only used by a few models/datasets, which are optional. See comments in `requirements.txt`.

## 3. ğŸ’¾ Prepare Datasets

### 3.1 Regular

Get them from [[Google Drive]](https://drive.google.com/drive/folders/13Cg1KYOlzM5C7K8gK8NfC-F3EYxkM3D2) provided by [Time-Series-Library](https://github.com/thuml/Time-Series-Library), which includes the following datasets in this repository:

- ECL (electricity)
- ETTh1 (ETT-small)
- ETTm1 (ETT-small)
- ILI (illness)
- Traffic (traffic)
- Weather (weather)

And place them under `storage/datasets` folder of this project (create the folder if not exists, or you can use symbolic link `ln -s` to redirect to existing dataset files).

You will get the following file structure under `storage/datasets`:

```
.
â”œâ”€â”€ electricity/
â”‚   â””â”€â”€ electricity.csv
â”œâ”€â”€ ETT-small/
â”‚   â”œâ”€â”€ ETTh1.csv
â”‚   â”œâ”€â”€ ETTh2.csv
â”‚   â”œâ”€â”€ ETTm1.csv
â”‚   â””â”€â”€ ETTm2.csv
â”œâ”€â”€ illness/
â”‚   â””â”€â”€ national_illness.csv
â”œâ”€â”€ traffic/
â”‚   â””â”€â”€ traffic.csv
â””â”€â”€ weather/
    â””â”€â”€ weather.csv
```

### 3.2 Irregular

#### 3.2.1 Human Activity

No need to prepare in advance.
Our code will automatically download then preprocess it if you want to train on it.

The following file structure will be found under `storage/datasets`, after the code finish preprocessing:
```
.
â””â”€â”€ HumanActivity/
    â”œâ”€â”€ processed/
    â”‚   â””â”€â”€ data.pt
    â””â”€â”€ raw/
        â””â”€â”€ ConfLongDemo_JSI.txt
```

#### 3.2.2 MIMIC III

Since MIMIC III requires credentialed access:
- Request for raw data from [here](https://physionet.org/content/mimiciii/1.4/). Files can be put wherever you like, and you don't have to extract `.csv.gz` as `.csv`.
- Data preprocessing

    Choose one of the options:

    - Option 1: Use the revised scripts in PyOmniTS.
        - Create a new virtual environment (only used in data preprocessing, not subsequent training) with Python 3.7, numpy 1.21.6, and pandas 1.3.5

            ```shell
            conda create -n python37 python=3.7
            conda activate python37
            pip install numpy==1.21.6 pandas==1.3.5
            ```
        - `python data/dependencies/MIMIC_III/preprocess/0_run_all.py`
    - Option 2: Use the original scripts in gru_ode_bayes.
        - Follow the processing scripts in [gru_ode_bayes](https://github.com/edebrouwer/gru_ode_bayes/tree/master/data_preproc/MIMIC) to get `complete_tensor.csv`.
        - Put the result under `~/.tsdm/rawdata/MIMIC_III_DeBrouwer2019/complete_tensor.csv`.

The following file structure will be found under `~/.tsdm`, after the code finish preprocessing (Note: `.parquet` files will be generated automatically after training any model on this dataset):
```
.
â”œâ”€â”€ datasets/
â”‚   â””â”€â”€ MIMIC_III_DeBrouwer2019/
â”‚       â”œâ”€â”€ metadata.parquet
â”‚       â””â”€â”€ timeseries.parquet
â””â”€â”€ rawdata/
    â””â”€â”€ MIMIC_III_DeBrouwer2019/
        â””â”€â”€ complete_tensor.csv
```


#### 3.2.2 MIMIC IV

Since MIMIC IV requires credentialed access:
- Request for raw data from [here](https://physionet.org/content/mimiciv/1.0/). Files can be put wherever you like, and you don't have to extract `.csv.gz` as `.csv`.
- Data preprocessing

    Choose one of the options:

    - Option 1: Use the revised scripts in PyOmniTS.
        - Create a new virtual environment (only used in data preprocessing, not subsequent training) with Python 3.8, numpy 1.24.4, and pandas 2.0.3

            ```shell
            conda create -n python38 python=3.8
            conda activate python38
            pip install numpy==1.24.4 pandas==2.0.3
            ```
        - `python data/dependencies/MIMIC_IV/preprocess/0_run_all.py`
    - Option 2: Use the original scripts in NeuralFlows.
        - Follow the processing scripts in [NeuralFlows](https://github.com/mbilos/neural-flows-experiments/blob/master/nfe/experiments/gru_ode_bayes/data_preproc) to get `full_dataset.csv`.
        - Put the result under `~/.tsdm/rawdata/MIMIC_IV_Bilos2021/full_dataset.csv`.

The following file structure will be found under `~/.tsdm`, after the code finish preprocessing (Note: `.parquet` files will be generated automatically after training any model on this dataset):
```
.
â”œâ”€â”€ datasets/
â”‚   â””â”€â”€ MIMIC_IV_Bilos2021/
â”‚       â””â”€â”€ timeseries.parquet
â””â”€â”€ rawdata/
    â””â”€â”€ MIMIC_IV_Bilos2021/
        â””â”€â”€ full_dataset.csv
```


#### 3.2.3 PhysioNet'12

No need to prepare in advance.
Our code will automatically download then preprocess it if you want to train on it.

The following file structure will be found under `~/.tsdm`, after the code finish preprocessing:
```
.
â”œâ”€â”€ datasets/
â”‚   â””â”€â”€ Physionet2012/
â”‚       â”œâ”€â”€ Physionet2012-set-A-sparse.tar
â”‚       â”œâ”€â”€ Physionet2012-set-B-sparse.tar
â”‚       â””â”€â”€ Physionet2012-set-C-sparse.tar
â””â”€â”€ rawdata/
    â””â”€â”€ Physionet2012/
        â”œâ”€â”€ set-a.tar.gz
        â”œâ”€â”€ set-b.tar.gz
        â””â”€â”€ set-c.tar.gz
```

#### 3.2.4 USHCN

No need to prepare in advance.
Our code will automatically download then preprocess it if you want to train on it.

The following file structure will be found under `~/.tsdm`, after the code finish preprocessing:
```
.
â”œâ”€â”€ datasets/
â”‚   â””â”€â”€ USHCN_DeBrouwer2019/
â”‚       â””â”€â”€ USHCN_DeBrouwer2019.parquet
â””â”€â”€ rawdata/
    â””â”€â”€ USHCN_DeBrouwer2019/
        â””â”€â”€ small_chunked_sporadic.csv
```

## 4. ğŸ“‚ (Optional) Folder Structure

You can optionally learn how PyOmniTS organize its folder structure:

```
.
â”œâ”€â”€ configs/ # (Auto-generated) YAML configs for experiments. Only saved as references, not input parameters.
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ data_provider/
|   |   â”œâ”€â”€ datasets/ # Main classes of datasets. File names match the string provided in --dataset_name.
|   |   â””â”€â”€ data_factory.py # Provides an interface to get torch.utils.data.Dataset and torch.utils.data.DataLoader
|   â””â”€â”€ dependencies/ # Dependencies for dataset classes under data/data_provider/datasets/
â”œâ”€â”€ docs # Documentations
â”œâ”€â”€ exp/
|   â”œâ”€â”€ exp_basic.py # Parent class for experiments.
|   â””â”€â”€ exp_main.py # Main class for experiments, inherit from the class in exp_basic.py
â”œâ”€â”€ layers/ # Dependencies for model classes under models/
â”œâ”€â”€ logs/ # (Auto-generated) Auto-rotated logs when running experiments.
â”œâ”€â”€ loss_fns/ # Main classes of loss functions. File names match the string provided in --loss.
â”œâ”€â”€ lr_schedulers/ # Main classes of some learning rate schedulers.
â”œâ”€â”€ models/ # Main classes of models. File names match the string provided in --model_name.
â”œâ”€â”€ scripts/ # Launch scripts for experiments.
â”œâ”€â”€ storage/ # (Auto-generated) General purpose storage folder, not recorded by git.
|   â”œâ”€â”€ datasets/ # Time series data for some datasets.
|   â””â”€â”€ results/ # Experiment results.
â”œâ”€â”€ tests/ # Unit tests only used by PyOmniTS maintainers.
â”œâ”€â”€ utils/
|   â”œâ”€â”€ configs.py # Command line arguments accepted by main.py
|   â”œâ”€â”€ ExpConfigs.py # Dataclass that wraps utils/configs.py for typo check. Passed to models, datasets, loss_fns,... for their initializations.
|   â”œâ”€â”€ globals.py # A few global variables (logger, accelerator,...).
|   â”œâ”€â”€ metrics.py # Calculate metrics (e.g., MSE) during testing.
|   â””â”€â”€ tools.py # misc helper functions and classes.
â”œâ”€â”€ wandb/ # (Auto-generated) Weight & Bias logs when --wandb 1 or --sweep 1.
â”œâ”€â”€ .all-contributorsrc # Only used in README.md.
â”œâ”€â”€ .gitignore # Git ignore rules.
â”œâ”€â”€ .python-version # Recommended Python version, display only.
â”œâ”€â”€ LICENSE # MIT License.
â”œâ”€â”€ main.py # Main entrance for experiments.
â”œâ”€â”€ pyproject.toml # Standard configuration file for Python projects.
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.lock # Python package requirements (with versions).
â”œâ”€â”€ requirements.txt # Python package requirements (without versions).
â”œâ”€â”€ run_unittest.sh # Launch script for unit tests in tests/. Only used by PyOmniTS maintainers.
â””â”€â”€ run.sh # Launch script for scripts/. Useful when launching multiple experiments at once.
```

Core logic when running experiments:

`scripts/` â†’ `main.py` â†’ `exp/exp_main.py`

## 5. ğŸ”¥ Training

Training scripts are located in `scripts` folder.
For example, to train mTAN on dataset Human Activity:

```shell
sh scripts/mTAN/HumanActivity.sh
```

Training results will be organized in `storage/results/${DATASET_NAME}/${DATASET_ID}/${MODEL_NAME}/${MODEL_ID}/${SEQ_LEN}_${PRED_LEN}/%Y_%m%d_%H%M/iter0`

## 6. â„ï¸ Testing

Testing will be automatically conducted once the training finished. 
If you wish to run test only, change command line argument `--is_training` in training script from `1` to `0` and run the script.

Testing result `metric.json` will be saved in `storage/results/${DATASET_NAME}/${DATASET_ID}/${MODEL_NAME}/${MODEL_ID}/${SEQ_LEN}_${PRED_LEN}/%Y_%m%d_%H%M/iter0/eval_%Y_%m%d_%H%M`
